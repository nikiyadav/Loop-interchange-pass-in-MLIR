From d0d9c4766d365e1a9ccc1c52a9f2411d3e112b66 Mon Sep 17 00:00:00 2001
From: Nikita <pga.nikita@gmail.com>
Date: Mon, 15 Jun 2020 16:35:24 +0530
Subject: [PATCH] loop-interchange-pass

---
 mlir/include/mlir/Analysis/AffineAnalysis.h   |   6 +
 mlir/include/mlir/Dialect/Affine/Passes.h     |   6 +-
 mlir/include/mlir/Dialect/Affine/Passes.td    |   7 +
 mlir/lib/Analysis/AffineAnalysis.cpp          |  35 +-
 .../Dialect/Affine/Transforms/CMakeLists.txt  |   1 +
 .../Affine/Transforms/LoopInterchange.cpp     | 639 ++++++++++++++++++
 mlir/lib/Dialect/Affine/Transforms/README.txt |  78 +++
 mlir/lib/Transforms/Utils/LoopUtils.cpp       |   2 +
 8 files changed, 771 insertions(+), 3 deletions(-)
 create mode 100644 mlir/lib/Dialect/Affine/Transforms/LoopInterchange.cpp
 create mode 100644 mlir/lib/Dialect/Affine/Transforms/README.txt

diff --git a/mlir/include/mlir/Analysis/AffineAnalysis.h b/mlir/include/mlir/Analysis/AffineAnalysis.h
index b421eee9514..652f8ae1f71 100644
--- a/mlir/include/mlir/Analysis/AffineAnalysis.h
+++ b/mlir/include/mlir/Analysis/AffineAnalysis.h
@@ -126,6 +126,12 @@ void getDependenceComponents(
     AffineForOp forOp, unsigned maxLoopDepth,
     std::vector<SmallVector<DependenceComponent, 2>> *depCompsVec);
 
+// Loop-interchange-pass
+void getAccessConstraints(
+  Operation *srcloadOrStoreOp, Operation *dstloadOrStoreOp, 
+  FlatAffineConstraints *dependenceConstraints);
+
 } // end namespace mlir
 
+
 #endif // MLIR_ANALYSIS_AFFINE_ANALYSIS_H
diff --git a/mlir/include/mlir/Dialect/Affine/Passes.h b/mlir/include/mlir/Dialect/Affine/Passes.h
index 0d7c3be240c..2a5da1ad1ca 100644
--- a/mlir/include/mlir/Dialect/Affine/Passes.h
+++ b/mlir/include/mlir/Dialect/Affine/Passes.h
@@ -33,8 +33,7 @@ std::unique_ptr<OperationPass<FuncOp>> createSimplifyAffineStructuresPass();
 
 /// Creates a loop invariant code motion pass that hoists loop invariant
 /// operations out of affine loops.
-std::unique_ptr<OperationPass<FuncOp>>
-createAffineLoopInvariantCodeMotionPass();
+std::unique_ptr<OperationPass<FuncOp>> createAffineLoopInvariantCodeMotionPass();
 
 /// Performs packing (or explicit copying) of accessed memref regions into
 /// buffers in the specified faster memory space through either pointwise copies
@@ -75,6 +74,9 @@ createSuperVectorizePass(ArrayRef<int64_t> virtualVectorSize);
 /// Overload relying on pass options for initialization.
 std::unique_ptr<OperationPass<FuncOp>> createSuperVectorizePass();
 
+// Creates a loop interchange pass to optimize for locality and parallelism
+std::unique_ptr<OperationPass<FuncOp>> createLoopInterchangePass();
+
 } // end namespace mlir
 
 #endif // MLIR_DIALECT_AFFINE_RANSFORMS_PASSES_H
diff --git a/mlir/include/mlir/Dialect/Affine/Passes.td b/mlir/include/mlir/Dialect/Affine/Passes.td
index 06e0920413a..c705f8d4275 100644
--- a/mlir/include/mlir/Dialect/Affine/Passes.td
+++ b/mlir/include/mlir/Dialect/Affine/Passes.td
@@ -118,4 +118,11 @@ def SimplifyAffineStructures : FunctionPass<"simplify-affine-structures"> {
   let constructor = "mlir::createSimplifyAffineStructuresPass()";
 }
 
+// Creates FunctionPass 'AffineLoopInterchange', triggered by the command line flag 
+// -affine-loop-interchange
+def AffineLoopInterchange : FunctionPass<"affine-loop-interchange"> {
+       let summary = "interchange loops to improve locality";
+       let constructor = "mlir::createLoopInterchangePass()";
+}
+
 #endif // MLIR_DIALECT_AFFINE_PASSES
diff --git a/mlir/lib/Analysis/AffineAnalysis.cpp b/mlir/lib/Analysis/AffineAnalysis.cpp
index 185be49930b..d377ae7acad 100644
--- a/mlir/lib/Analysis/AffineAnalysis.cpp
+++ b/mlir/lib/Analysis/AffineAnalysis.cpp
@@ -763,7 +763,7 @@ DependenceResult mlir::checkMemrefAccessDependence(
     unsigned loopDepth, FlatAffineConstraints *dependenceConstraints,
     SmallVector<DependenceComponent, 2> *dependenceComponents, bool allowRAR) {
   LLVM_DEBUG(llvm::dbgs() << "Checking for dependence at depth: "
-                          << Twine(loopDepth) << " between:\n";);
+                         << Twine(loopDepth) << " between:\n";);
   LLVM_DEBUG(srcAccess.opInst->dump(););
   LLVM_DEBUG(dstAccess.opInst->dump(););
 
@@ -815,6 +815,7 @@ DependenceResult mlir::checkMemrefAccessDependence(
 
   initDependenceConstraints(srcDomain, dstDomain, srcAccessMap, dstAccessMap,
                             valuePosMap, dependenceConstraints);
+  
 
   assert(valuePosMap.getNumDims() ==
          srcDomain.getNumDimIds() + dstDomain.getNumDimIds());
@@ -826,6 +827,8 @@ DependenceResult mlir::checkMemrefAccessDependence(
                                         dependenceConstraints)))
     return DependenceResult::Failure;
 
+  LLVM_DEBUG(dependenceConstraints->dump(););
+
   // Add 'src' happens before 'dst' ordering constraints.
   addOrderingConstraints(srcDomain, dstDomain, loopDepth,
                          dependenceConstraints);
@@ -882,3 +885,33 @@ void mlir::getDependenceComponents(
     }
   }
 }
+
+// Adds equality constraints that equate src and dst access functions
+// represented by 'srcAccessMap' and 'dstAccessMap' for each result.
+void mlir::getAccessConstraints(Operation *srcloadOrStoreOp,
+                                Operation *dstloadOrStoreOp,
+                                FlatAffineConstraints *constraints) {
+  MemRefAccess srcAccess(srcloadOrStoreOp);
+  // Get composed access function for 'srcAccess'
+  AffineValueMap srcAccessMap;
+  srcAccess.getAccessMap(&srcAccessMap);
+  MemRefAccess dstAccess(dstloadOrStoreOp);
+  // Get composed access function for 'dstAccess'
+  AffineValueMap dstAccessMap;
+  dstAccess.getAccessMap(&dstAccessMap);
+  // Get iteration domain for the 'srcAccess' operation.
+  FlatAffineConstraints srcDomain;
+  getInstIndexSet(srcAccess.opInst, &srcDomain);
+  // Get iteration domain for the 'dstAccess' operation.
+  FlatAffineConstraints dstDomain;
+  getInstIndexSet(dstAccess.opInst, &dstDomain);
+  // Build dim and symbol position maps for each access from access operand
+  // Value to position in constraint system.
+  ValuePositionMap valuePosMap;
+  buildDimAndSymbolPositionMaps(srcDomain, dstDomain, srcAccessMap,
+                                dstAccessMap, &valuePosMap, constraints);
+  initDependenceConstraints(srcDomain, dstDomain, srcAccessMap, dstAccessMap,
+                            valuePosMap, constraints);
+  addMemRefAccessConstraints(srcAccessMap, dstAccessMap, valuePosMap,
+                             constraints);
+}
\ No newline at end of file
diff --git a/mlir/lib/Dialect/Affine/Transforms/CMakeLists.txt b/mlir/lib/Dialect/Affine/Transforms/CMakeLists.txt
index bcad44d7249..215cc49fe53 100644
--- a/mlir/lib/Dialect/Affine/Transforms/CMakeLists.txt
+++ b/mlir/lib/Dialect/Affine/Transforms/CMakeLists.txt
@@ -6,6 +6,7 @@ add_mlir_dialect_library(MLIRAffineTransforms
   LoopUnrollAndJam.cpp
   SuperVectorize.cpp
   SimplifyAffineStructures.cpp
+  LoopInterchange.cpp
 
   ADDITIONAL_HEADER_DIRS
   ${MLIR_MAIN_INCLUDE_DIR}/mlir/Dialect/Affine
diff --git a/mlir/lib/Dialect/Affine/Transforms/LoopInterchange.cpp b/mlir/lib/Dialect/Affine/Transforms/LoopInterchange.cpp
new file mode 100644
index 00000000000..f258054b3af
--- /dev/null
+++ b/mlir/lib/Dialect/Affine/Transforms/LoopInterchange.cpp
@@ -0,0 +1,639 @@
+//===- LoopInterchange.cpp - loop interchange pass  driven by an analytical cost model that
+//     optimizes for locality (spatial, temporal - both self and group) and parallelism
+//     for multicores (so as to minimize the frequence of synchronization)---------===//
+//
+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
+// See https://llvm.org/LICENSE.txt for license information.
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+//
+//===----------------------------------------------------------------------===//
+#include "PassDetail.h"
+#include "mlir/Dialect/Affine/Passes.h"
+#include "mlir/IR/AffineMap.h"
+#include "mlir/IR/BlockAndValueMapping.h"
+#include "mlir/IR/Builders.h"
+#include "llvm/ADT/DenseMap.h"
+#include "llvm/Support/CommandLine.h"
+#include "mlir/Analysis/AffineAnalysis.h"
+#include "mlir/Dialect/Affine/IR/AffineOps.h" 
+#include "mlir/Transforms/LoopUtils.h"  
+#include "mlir/IR/AffineExpr.h"
+#include "mlir/Analysis/AffineStructures.h" 
+#include "mlir/Analysis/LoopAnalysis.h" 
+#include "mlir/Dialect/Affine/IR/AffineValueMap.h"
+#include "mlir/IR/Types.h"
+#include "mlir/IR/OpImplementation.h"
+#include "mlir/Analysis/Utils.h"
+#include "mlir/IR/Operation.h"
+#include "llvm/Support/Debug.h"
+#include "llvm/Support/raw_ostream.h"
+#include "mlir/Support/LLVM.h"
+#include <bits/stdc++.h>
+
+using namespace mlir;
+using namespace std;
+
+#define DEBUG_TYPE "affine-loop-interchange"
+
+namespace {
+struct LoopInterchange : public AffineLoopInterchangeBase<LoopInterchange> {
+  // Chosen cache line size for evaluating cost model.
+  int cacheLineSize = 8;
+
+  void runOnFunction() override;
+  LogicalResult runOnAffineForOp(AffineForOp forOp);
+
+  std::vector<std::vector<unsigned>> findValidLoopPermutations(AffineForOp);
+
+  template <typename LoadOrStoreOpPointer>
+  std::vector<std::vector<int>> getAccessMatrix(LoadOrStoreOpPointer memoryOp);
+
+  map<unsigned, vector<vector<Operation *>>>
+  getReuseGroupsForEachLoop(AffineForOp forOp);
+  vector<unsigned> getSpatialReuse(std::vector<std::vector<int>> accessMatrix);
+  vector<unsigned> getTemporalReuse(std::vector<std::vector<int>> accessMatrix);
+  vector<unsigned> getParallelLoops(AffineForOp forOp);
+
+  template <typename LoadOrStoreOpPointer>
+  bool hasGroupSpatialReuse(LoadOrStoreOpPointer srcOpInst,
+                            LoadOrStoreOpPointer dstOpInst);
+  template <typename LoadOrStoreOpPointer>
+  bool hasGroupTemporalReuse(LoadOrStoreOpPointer srcOpInst,
+                             LoadOrStoreOpPointer dstOpInst, unsigned loop);
+  template <typename LoadOrStoreOpPointer>
+  bool hasSelfSpatialReuse(LoadOrStoreOpPointer memoryOp, unsigned loop);
+  template <typename LoadOrStoreOpPointer>
+  bool hasSelfTemporalReuse(LoadOrStoreOpPointer memoryOp, unsigned loop);
+};
+} // end anonymous namespace
+
+std::unique_ptr<OperationPass<FuncOp>> mlir::createLoopInterchangePass() {
+  return std::make_unique<LoopInterchange>();
+}
+
+void LoopInterchange::runOnFunction() {
+  FuncOp f = getFunction();
+  SmallVector<AffineForOp, 4> loops; // stores forOps from outermost to innermost
+  f.walk([&](AffineForOp forOp) {
+    if (getNestingDepth(forOp) == 0) {
+      loops.insert(loops.begin(), forOp);
+      // llvm::outs() << "For loop\n";
+      // for(auto it=loops.begin();it!=loops.end();it++) { (*it).dump(); }
+      ArrayRef<AffineForOp> loops_arrayRef = makeArrayRef(loops);
+      if (isPerfectlyNested(loops_arrayRef)) {
+        runOnAffineForOp(forOp);
+      }
+      else {
+        // llvm::outs() << "Loops are not perfectly nested\n";
+      }
+      loops.clear();
+      // if(loops.empty()) { llvm::outs() << "loops is empty\n"; }
+    }
+    else {
+      loops.insert(loops.begin(), forOp);
+    }
+  });
+}
+
+//1. For each loop l, compute number of memory accesses made when l is the innermost loop.
+//        For innermost Loop, number of memory accesses = {
+//            1 , when reference is loop invariant;
+//            (tripCount/cacheLineSize), when reference has spatial reuse for loop l;
+//            (tripCount), otherwise
+//       }
+//        a. For each reference group, choose a reference R.
+//            i. if R has spatial reuse on loop l, add (tripCount/cacheLineSize) to number of memory accesses.
+//            ii. else, if R has temporal reuse on loop l, add 1 to number of memory accesses.
+//            iii. else, (add tripCount) to number of memory accesses.
+//        b. Multiple the result of number of memory accesses by the tripCount of all the remaining loops.
+// 2. Choose the loop with least number of memory accesses as the innermost loop, say it is L.
+// 3. Find the valid loop permutation which has loop L as the innermost loop.
+// 4. Find the loops which are parallel, does not carry any loop dependence.
+// 5. For each loop permutation found in step 5, calculate the cost of synchronization.
+//        Cost of synchronization is calculated for each parallel loop.
+//        For a loop, synchronization cost = product of tripCounts of all loops which are at outer positions to this loop.
+//6. Choose the permutation with the least synchronization cost as the best permutation.
+LogicalResult LoopInterchange::runOnAffineForOp(AffineForOp forOp) {
+  map<unsigned, vector<vector<Operation *>> > loop_refGroups = getReuseGroupsForEachLoop(forOp);
+
+  SmallVector<AffineForOp, 4> loops;
+  getPerfectlyNestedLoops(loops, forOp);
+  unsigned loopDepth = loops.size();
+
+  // tripCount of each loop
+  vector<unsigned long long> tripCount(loopDepth, INT_MAX);
+  for (int d=0;d<loopDepth;d++) {
+    auto maybeConstantCount = getConstantTripCount(loops[d]);
+    if (maybeConstantCount)
+      tripCount[d] = maybeConstantCount.getValue(); 
+  }
+
+  // find valid permutations
+  vector<vector<unsigned>> validLoopPerm;
+  validLoopPerm = findValidLoopPermutations(forOp);
+  if (validLoopPerm.size() <= 1) return success();
+
+  vector<int> validInnerLoop(loopDepth, 0);
+  for(int i=0;i<validLoopPerm.size();i++) {
+    vector<unsigned> loopPerm = validLoopPerm[i];
+    int innerLoop = loopPerm[loopPerm.size()-1];
+    validInnerLoop[innerLoop] = 1;
+  }
+
+  vector<unsigned long long> numMemAccess(loopDepth, 0);
+  for(unsigned d=0;d<loopDepth;d++) { 
+    // proceeds only for valid innermost loops
+    //  if not valid then set numMemAccess[d] to LONG_LONG_MAX
+    if (!validInnerLoop[d]) {
+      numMemAccess[d] = LONG_LONG_MAX;
+      continue;
+    }
+    unsigned long long trip = tripCount[d];  //get trip count
+    // considering d as the innermost loop
+    // using the ref groups calculated for this loop
+    vector<vector<Operation *>> refGroup = loop_refGroups[d];
+    int sz = refGroup.size();
+    for(int i=0;i<sz;i++) {
+      vector<Operation*> currGroup = refGroup[i];
+      // take one member of this group and do numMemAcceses calculation based on that.
+      Operation *opInst = currGroup[0];
+      // cost function for innermost loop: {
+      // 1            ,for loop invariant references,
+      // (trip/cls)   ,for references with spatial reuse for this loop,
+      // (trip)       ,otherwise.
+      // }
+      if (hasSelfTemporalReuse(opInst, d))
+        numMemAccess[d] += 1;
+      else if (hasSelfSpatialReuse(opInst, d)) 
+        numMemAccess[d] += (trip/cacheLineSize);
+      else
+        numMemAccess[d] += trip;
+    }
+    // to get correct numMemAccess: multiply numMemAccess[d] by tripCounts of
+    // all remaining loops
+    for (unsigned dd=0;dd<loopDepth;dd++) {
+      if (dd == d) {
+        continue;
+      }
+      numMemAccess[d] *= tripCount[dd];
+    }
+  }
+  // step 1: choose the loop with least number of memory accesses (numMemAccess)
+  // as innermost Loop. step 2: compute synchronization cost for parallel loops.
+
+  // choose the least cost loop as innermost Loop.
+  unsigned long long leastCost1 = numMemAccess[0]; 
+  unsigned leastCostLoop = 0;
+  for (unsigned d=0;d<loopDepth;d++) {
+    if (numMemAccess[d]<leastCost1) {
+      leastCost1 = numMemAccess[d];
+      leastCostLoop = d;
+    }
+  }
+  // print
+  // llvm::outs() << "least cost loop = " << leastCostLoop << "\n";
+
+  // get the valid permutations which has leastCostLoop as innermost
+  vector<vector<unsigned> > validPerms;
+  for(int i=0;i<validLoopPerm.size();i++) {
+    vector<unsigned> loopPerm = validLoopPerm[i];
+    int innerLoop = loopPerm[loopPerm.size()-1];
+    if (innerLoop == leastCostLoop) {
+      validPerms.push_back(loopPerm);
+    }
+  }
+  
+  int totalPerm = validPerms.size();
+  // getParallelLoops returns an array of size loopDepth where each entry is either 0 or 1. 0 - not parallel, 1 - parallel loop
+  vector<unsigned> parallelLoopMap = getParallelLoops(forOp);
+
+  // print
+  // llvm::outs() << "parallel loops = ";
+  // for (int i=0;i<parallelLoopMap.size();i++) {
+  //   if (parallelLoopMap[i])
+  //     llvm::outs() << i << " ";
+  // }
+  // llvm::outs() << "\n";
+
+  // computing synchronization Cost for each valid permutation with
+  // leastCostLoop as innermost loop. Synchronization cost for a parallel loop l
+  // = product of tripcount of loops which are at outer positions than loop l.
+  vector<unsigned long long> syncCost(totalPerm, 0);
+  for (int v=0; v < totalPerm; v++) {
+    vector<unsigned> loopPerm = validPerms[v];
+    for (unsigned i=0;i<parallelLoopMap.size();i++) {
+      if (parallelLoopMap[i]) {
+        // loop i is parallel
+        unsigned long long cost = 1;
+        unsigned parallelLoop = i;
+        if (parallelLoop == leastCostLoop) continue;
+        for(int j=0;j<loopPerm.size()-1;j++) {
+          if (loopPerm[j] != parallelLoop) {
+            cost *= tripCount[loopPerm[j]];
+          }
+          else if (loopPerm[j] == parallelLoop) {
+            break;
+          }
+        }
+        if (cost != 1) {
+          syncCost[v] += cost;
+        }
+      }
+    }
+  }
+
+  // step 3: Choose best loop permutation to be the one with least
+  // synchronization cost. print llvm::outs() << "synCost = \n";
+  unsigned leastSyncCostPerm = 0;
+  unsigned leastSyncCost = syncCost[0];
+  for(unsigned v=0;v<totalPerm;v++) {
+    // for(int i=0;i<validPerms[v].size();i++) {
+    // llvm::outs() << validPerms[v][i] << " ";
+    // }
+    // llvm::outs() << " => " << syncCost[v];
+    // llvm::outs() << "\n";
+    if (syncCost[v] < leastSyncCost) {
+      leastSyncCostPerm = v;
+      leastSyncCost = syncCost[v];
+    }
+  }
+
+  vector<unsigned> bestPerm = validPerms[leastSyncCostPerm];
+  // print best perm
+  // llvm::outs() << "best permutation = ";
+  // for(int i=0;i<bestPerm.size();i++) {
+  //   llvm::outs() << bestPerm[i] << " ";
+  // }
+  // llvm::outs() << "\n";
+
+  // step 4: create permutation map
+  SmallVector<unsigned, 4> loopPermMap(loopDepth);
+  for (int j=0;j<bestPerm.size();j++) {
+    loopPermMap[bestPerm[j]] = j;
+  }
+
+  // llvm::outs() << "loopPermMap =";
+  // for (int j=0;j<loopPermMap.size();j++) {
+  //   llvm::outs() << loopPermMap[j] << " ";
+  // }
+  // llvm::outs() << "\n";
+  // Perform loop interchange according to permutation 'loopPermMap'.
+  unsigned loopNestRootIndex = permuteLoops(loops, loopPermMap);
+  return success();
+}
+
+bool equalMatrices(vector<vector<int>> srcAccessMatrix,vector<vector<int>> dstAccessMatrix) {
+  int numRows = srcAccessMatrix.size();
+  int numCols = srcAccessMatrix[0].size();
+  if ((numRows != dstAccessMatrix.size()) || (numCols != dstAccessMatrix[0].size())) 
+    return false;
+
+  for( int r=0;r<numRows;r++) {
+    for(int c=0;c<numCols;c++) {
+      if (srcAccessMatrix[r][c] != dstAccessMatrix[r][c])
+        return false;
+    }
+  }
+  return true;
+}
+
+// For each loop l, compute groups of array references.
+//        Two references ref1 and ref2 belong to same group with respect to loop
+//        l if:
+//            a. they refer to the same array and has exactly same access
+//            function. b. or, they refer to the same array and differ only in
+//            lth dimension by atmost cacheLineSize. c. or, they refer to the
+//            same array and differ by at most cacheLineSize in the last
+//            dimension.
+// conditions (a) and (b) corresponds to group temporal reuse, (c) corresponds
+// to group spatial reuse.
+map<unsigned, vector<vector<Operation* >> > LoopInterchange::getReuseGroupsForEachLoop(AffineForOp forOp) {
+  // get all load and store operations
+  SmallVector<Operation *, 8> loadAndStoreOpInsts;
+  map<Operation *, bool> visitedOp;
+  forOp.getOperation()->walk([&](Operation *opInst) {
+    if (isa<AffineLoadOp>(opInst) || isa<AffineStoreOp>(opInst)) {
+      loadAndStoreOpInsts.push_back(opInst);
+      visitedOp[opInst] = false;
+    }
+  });
+  SmallVector<AffineForOp, 4> loops;
+  getPerfectlyNestedLoops(loops, forOp);
+  unsigned loopDepth = loops.size();
+
+  // get groups of loadAndStoreOpInsts for each loop
+  map<unsigned, vector<vector<Operation* >> > loop_refGroups; 
+  /* map: key is loop (e.g.if loop  = i,j,k then key = 0 for i, key = 1 for j, key = 2 for k)
+    value is collection of refGroups for the loop
+  */
+  for (unsigned d = 0; d < loopDepth; d++) {
+    vector<vector<Operation* >> refGroups;
+    unsigned numOps = loadAndStoreOpInsts.size();
+    // mark all ops as unvisited;
+    for (unsigned i = 0; i < numOps; i++) {
+      visitedOp[loadAndStoreOpInsts[i]] = false;
+    }
+    for (unsigned i = 0; i < numOps; ++i) {
+      auto *srcOpInst = loadAndStoreOpInsts[i];
+      if (visitedOp[srcOpInst]) continue; //already added to a group
+      // create a group and mark visited
+      visitedOp[srcOpInst] = true;
+      vector<Operation *> currGroup;
+      currGroup.push_back(srcOpInst);
+      Value srcArray; // src array name
+      if (auto store = dyn_cast<AffineStoreOp>(srcOpInst)) {
+        srcArray = srcOpInst->getOperand(1);
+      }
+      else if (auto load = dyn_cast<AffineLoadOp>(srcOpInst)) {
+        srcArray = srcOpInst->getOperand(0);
+      }
+      for (unsigned j = 0; j < numOps; ++j) {
+        auto *dstOpInst = loadAndStoreOpInsts[j];
+        if ((i == j) || visitedOp[dstOpInst] == true) {
+          // same operation or already added to group
+          continue;
+        }
+        Value dstArray; // dst array name
+        if (auto store = dyn_cast<AffineStoreOp>(dstOpInst)) {
+          dstArray = dstOpInst->getOperand(1);
+        }
+        else if (auto load = dyn_cast<AffineLoadOp>(dstOpInst)) {
+          dstArray = dstOpInst->getOperand(0);
+        }
+        if (srcArray != dstArray) {
+          continue;
+        }
+        else {
+          // refer to same array and dstOpInst is not visited
+          // check 1: they have same access matrix
+          // check 2: check 1 && has group temporal reuse for loop d if they only differ in subscript having loop d by a small constant (< cache line size).
+          // check 3: check 1 && has group spatial reuse if they differ in only last dimension
+          // if check 2 or check 3 is satisfied, add this op to currGroup and mark it visited.
+          vector<vector<int> > srcAccessMatrix = getAccessMatrix(srcOpInst);
+          vector<vector<int> > dstAccessMatrix = getAccessMatrix(dstOpInst);
+          if (equalMatrices(srcAccessMatrix, dstAccessMatrix)) {
+            // group spatial reuse
+            if (hasGroupSpatialReuse(srcOpInst, dstOpInst) || hasGroupTemporalReuse(srcOpInst, dstOpInst, d)) {
+              // hasGroupSpatialReuse handles the case when array references are exactly same. (eg. A[i,j] and A[i,j])
+              currGroup.push_back(dstOpInst);
+              visitedOp[dstOpInst] = true;
+            }
+          }
+        }
+      } 
+      refGroups.push_back(currGroup);
+    }
+    loop_refGroups[d] = refGroups;
+  }
+  return loop_refGroups;
+}
+
+template <typename LoadOrStoreOpPointer>
+bool LoopInterchange::hasGroupSpatialReuse(LoadOrStoreOpPointer srcOpInst, LoadOrStoreOpPointer dstOpInst) {
+  // int cacheLineSize = 8;
+  FlatAffineConstraints accessConstraints;
+  getAccessConstraints(srcOpInst, dstOpInst, &accessConstraints);
+  // accessConstraints.dump();
+  unsigned numCols = accessConstraints.getNumCols();
+  unsigned arrayDimension; // num of rows
+  if (auto load = dyn_cast<AffineLoadOp>(srcOpInst)) {
+    auto memRefType = load.getMemRef().getType().template cast<MemRefType> ();
+    arrayDimension = memRefType.getRank();
+  }
+  else if (auto store = dyn_cast<AffineStoreOp>(srcOpInst)) {
+    auto memRefType = store.getMemRef().getType().template cast<MemRefType> ();
+    arrayDimension = memRefType.getRank();
+  }
+  // true if constant term differs in only last dimension
+  for(unsigned r=0;r<arrayDimension;r++) {
+    if ((r < (arrayDimension-1)) && accessConstraints.atEq(r,numCols-1) != 0)
+      return false;
+    if ( (r == (arrayDimension-1)) && accessConstraints.atEq(r,numCols-1) < cacheLineSize)
+      return true;
+  }
+  return false;
+}
+
+template <typename LoadOrStoreOpPointer>
+bool LoopInterchange::hasGroupTemporalReuse(LoadOrStoreOpPointer srcOpInst, LoadOrStoreOpPointer dstOpInst, unsigned loop) {
+  // int cacheLineSize = 8;
+  FlatAffineConstraints accessConstraints;
+  getAccessConstraints(srcOpInst, dstOpInst, &accessConstraints);
+  // srcOpInst->dump(); //print
+  // dstOpInst->dump();
+  // accessConstraints.dump();
+  unsigned numCols = accessConstraints.getNumCols();
+  unsigned arrayDimension; // num of rows
+  if (auto load = dyn_cast<AffineLoadOp>(srcOpInst)) {
+    auto memRefType = load.getMemRef().getType().template cast<MemRefType> ();
+    arrayDimension = memRefType.getRank();
+  }
+  else if (auto store = dyn_cast<AffineStoreOp>(srcOpInst)) {
+    auto memRefType = store.getMemRef().getType().template cast<MemRefType> ();
+    arrayDimension = memRefType.getRank();
+  }
+  // find array dimension which is not invariant to loop: 
+  // these are the rows of access Matrix which has non-zero entries in loop column.
+
+  unsigned loopDepth = getNestingDepth(srcOpInst); // num of columns
+  std::vector< std::vector<int> > accessMatrix;
+  for (unsigned p=0;p<arrayDimension;p++) {
+    std::vector<int> tmp;
+    for (unsigned q=0;q<loopDepth;q++) {
+          tmp.push_back(accessConstraints.atEq(p,q));
+    }
+    accessMatrix.push_back(tmp);
+  }
+
+  vector<int> loopVariantDims(arrayDimension,0);
+  for (unsigned r=0;r<arrayDimension;r++) {
+    if (accessMatrix[r][loop] != 0) 
+      loopVariantDims[r] = 1;
+  }
+
+  // since access matrices are same, return true if constant terms differ in only loopVariantDims.
+  for(unsigned r=0;r<arrayDimension;r++) {
+    if (loopVariantDims[r] == 0 && accessConstraints.atEq(r,numCols-1) != 0)
+      return false;
+    if (loopVariantDims[r] == 1 && accessConstraints.atEq(r,numCols-1) > cacheLineSize)
+      return false;
+  }
+  return true;
+}
+
+template <typename LoadOrStoreOpPointer>
+std::vector<std::vector<int>> LoopInterchange::getAccessMatrix(LoadOrStoreOpPointer memoryOp) {
+  FlatAffineConstraints accessConstraints;
+  getAccessConstraints(memoryOp, memoryOp, &accessConstraints);
+  // memoryOp->dump();
+  // accessConstraints.dump(); // printing
+  unsigned loopDepth = getNestingDepth(memoryOp); // num of columns
+  unsigned arrayDimension; // num of rows
+  auto load = dyn_cast<AffineLoadOp>(memoryOp);
+  auto store = dyn_cast<AffineStoreOp>(memoryOp);
+  if (load) {
+    auto memRefType = load.getMemRef().getType().template cast<MemRefType> ();
+    arrayDimension = memRefType.getRank();
+  }
+  else if (store) {
+    auto memRefType = store.getMemRef().getType().template cast<MemRefType> ();
+    arrayDimension = memRefType.getRank();
+  }
+  // llvm::outs() << "AccessMatrix:\n";
+  std::vector< std::vector<int> > accessMatrix;
+  for (unsigned p=0;p<arrayDimension;p++) {
+    std::vector<int> tmp;
+    for (unsigned q=0;q<loopDepth;q++) {
+          tmp.push_back(accessConstraints.atEq(p,q));
+          // llvm::outs() << accessConstraints.atEq(p,q) << " ";
+    }
+    accessMatrix.push_back(tmp);
+    // llvm::outs() << "\n";
+  }
+  return accessMatrix;
+}
+
+template <typename LoadOrStoreOpPointer>
+bool LoopInterchange::hasSelfTemporalReuse(LoadOrStoreOpPointer memoryOp, unsigned loop) {
+  vector<vector<int>> accessMatrix = getAccessMatrix(memoryOp);
+  vector<unsigned> temporalReuseMap = getTemporalReuse(accessMatrix);
+  if (temporalReuseMap[loop] == 1)
+    return true;
+  return false;
+}
+
+template <typename LoadOrStoreOpPointer>
+bool LoopInterchange::hasSelfSpatialReuse(LoadOrStoreOpPointer memoryOp, unsigned loop) {
+  vector<vector<int>> accessMatrix = getAccessMatrix(memoryOp);
+  vector<unsigned> spatialReuseMap = getSpatialReuse(accessMatrix);
+  if (spatialReuseMap[loop] == 1)
+    return true;
+  return false;
+}
+
+// Returns a vector of size loopDepth, index i corresponds to loop i.
+// For a given accessMatrix, 0 indicates access is invariant to loop i.
+// 1 indicates access is variant to loop i.
+vector<unsigned> LoopInterchange::getTemporalReuse(std::vector<std::vector<int>> accessMatrix) {
+  // find zero columns
+  int numRows = accessMatrix.size();
+  int numCols = accessMatrix[0].size();
+  vector<unsigned> temporalReuseMap(numCols, 0);
+
+  for (int c = 0; c < numCols; c++) {
+    for (int r = 0; r < numRows; r++) {
+      if (accessMatrix[r][c] != 0) {
+        break;
+      }
+      else if ( (r == (numRows-1)) && (accessMatrix[r][c] == 0)) {
+        temporalReuseMap[c] = 1;
+      }
+    }
+  }
+  return temporalReuseMap;
+}
+
+// Returns a vector of size loopDepth, index i corresponds to loop i.
+// For a given accessMatrix, 0 indicates absence of spatial reuse for loop i.
+// 1 indicates presence of spatial reuse for loop i.
+vector<unsigned> LoopInterchange::getSpatialReuse(std::vector<std::vector<int>> accessMatrix) {
+  // find if there is a column in access matrix such that only last entry is
+  // non-zero and small. [0,0,0,....,c] int cacheLineSize = 8;
+  int numRows = accessMatrix.size();
+  int numCols = accessMatrix[0].size();
+  vector<unsigned> spatialReuseMap(numCols, 0);
+
+  for (int c = 0; c < numCols; c++) {
+    for (int r = 0; r < numRows; r++) {
+      if ((r != (numRows-1)) && (accessMatrix[r][c] != 0)) {
+        break;
+      }
+      else if ( (r == (numRows-1)) && (accessMatrix[r][c] != 0) && (accessMatrix[r][c] < cacheLineSize)) {
+        spatialReuseMap[c] = 1;
+      }
+    }
+  }
+  return spatialReuseMap;
+}
+
+// Function to display the array 
+void display(unsigned a[], unsigned n) 
+{ 
+    for (unsigned i = 0; i < n; i++) { 
+        std::cout << a[i] << "  "; 
+    } 
+    std::cout << std::endl; 
+} 
+
+// Given a forOp, returns all the loop permutations which have lexicographically positive dependence vectors.
+std::vector<std::vector<unsigned>> LoopInterchange::findValidLoopPermutations(AffineForOp forOp) {
+  SmallVector<AffineForOp, 4> loops;
+  getPerfectlyNestedLoops(loops,forOp);
+  std::vector<std::vector<unsigned>> validLoopPerm;
+  if (loops.size() < 2)
+    return validLoopPerm;  
+  unsigned maxLoopDepth = loops.size();
+  unsigned arr[maxLoopDepth];
+  for(unsigned i=0; i<maxLoopDepth; i++) {
+    arr[i] = i;
+  }
+  std::vector<unsigned> loopPermMap(maxLoopDepth);
+  vector<unsigned> loopPerm(maxLoopDepth);
+  do {
+    for (unsigned i = 0; i < maxLoopDepth; ++i) {
+      loopPermMap[arr[i]] = i; // inverted, referred sinkSequentialLoops func
+      loopPerm[i] = arr[i];    // not inverted
+    }
+    ArrayRef<AffineForOp> loops_arrayRef = makeArrayRef(loops);
+    ArrayRef<unsigned> loopPermMap_arrayRef = llvm::makeArrayRef(loopPermMap);
+    if ( isValidLoopInterchangePermutation(loops_arrayRef,loopPermMap_arrayRef) ) {
+      // display(arr,maxLoopDepth);
+      validLoopPerm.push_back(loopPerm); // not loopPermMap
+    }
+  } while(std::next_permutation(arr,arr+maxLoopDepth));
+  return validLoopPerm;
+}
+
+// returns a vector in which for an index i, 0 indicates loop i is not parallel and 1 indicates loop i is parallel.
+// A loop is parallel if it does not carry any dependence.
+vector<unsigned> LoopInterchange::getParallelLoops(AffineForOp forOp) {
+  SmallVector<AffineForOp, 4> loops;
+  getPerfectlyNestedLoops(loops,forOp);
+  unsigned maxLoopDepth = loops.size();
+
+  vector<unsigned> parallelLoops(maxLoopDepth, 1); // initially all parallel
+
+  std::vector<SmallVector<DependenceComponent, 2>> depCompsVec;
+  getDependenceComponents(loops[0], maxLoopDepth, &depCompsVec);
+  for (unsigned i = 0, e = depCompsVec.size(); i < e; ++i) {
+     SmallVector<DependenceComponent, 2> &depComps = depCompsVec[i];
+     assert(depComps.size() >= maxLoopDepth);
+     for (unsigned j = 0; j < maxLoopDepth; ++j) {
+       DependenceComponent &depComp = depComps[j];
+       assert(depComp.lb.hasValue() && depComp.ub.hasValue());
+       if (depComp.lb.getValue() != 0 || depComp.ub.getValue() != 0)
+       {
+         parallelLoops[j] = 0;
+         break;
+       }
+     }
+   }
+
+   // print
+   // llvm::outs() << "depCompsVec: \n";
+   // for (unsigned i = 0, e = depCompsVec.size(); i < e; ++i) {
+   //   std::cout << "depCompsVec[" << i << "] = ";
+   //   SmallVector<DependenceComponent, 2> &depComps = depCompsVec[i];
+   //   assert(depComps.size() >= maxLoopDepth);
+   //   for (unsigned j = 0; j < maxLoopDepth; ++j) {
+   //     DependenceComponent &depComp = depComps[j];
+   //     assert(depComp.lb.hasValue() && depComp.ub.hasValue());
+   //     //depComp.op->dump();
+   //     std::cout << "(" << depComp.lb.getValue() << ", " <<
+   //     depComp.ub.getValue() << ") ";
+   //   }
+   //   std::cout << "\n";
+   // }
+
+   return parallelLoops;
+}
\ No newline at end of file
diff --git a/mlir/lib/Dialect/Affine/Transforms/README.txt b/mlir/lib/Dialect/Affine/Transforms/README.txt
new file mode 100644
index 00000000000..d9507873204
--- /dev/null
+++ b/mlir/lib/Dialect/Affine/Transforms/README.txt
@@ -0,0 +1,78 @@
+Test case not passed:
+----------------------
+	Test case 9: Imperfect loop nest is not handled.
+
+=============================================================================
+
+Other test case outputs match the expected loop permutation, 
+but due to some errors in the Filecheck statements, some are not passed.
+Following are such test cases:
+
+Test case 2: interchange_for_spatial_temporal
+---------------------------------------------
+	Output of my pass:
+	------------------
+	func @interchange_for_spatial_temporal(%arg0: memref<2048xf64>) {
+	    affine.for %arg1 = 0 to 2048 {
+	      affine.for %arg2 = 0 to 2048 {
+		%0 = affine.load %arg0[%arg1] : memref<2048xf64>
+		%1 = affine.load %arg0[%arg1] : memref<2048xf64>
+		%2 = affine.load %arg0[%arg2] : memref<2048xf64>
+	      }
+	    }
+	    return
+	  }
+	
+	Original check statements:
+	// More reuse with %j, %i order.
+	// CHECK:       affine.load %arg0[%arg1] : memref<2048xf64>	
+	// CHECK-NEXT:  affine.load %0, %arg0[%arg1] : memref<2048xf64>
+	// CHECK-NEXT:  affine.load %0, %arg0[%arg2] : memref<2048xf64>
+
+	It is passed when check statements are modified:
+	// CHECK:       affine.load %arg0[%arg1] : memref<2048xf64>
+	// CHECK-NEXT:  affine.load %arg0[%arg1] : memref<2048xf64>
+	// CHECK-NEXT:  affine.load %arg0[%arg2] : memref<2048xf64>
+
+Test case 4: interchange_for_outer_parallelism
+----------------------------------------------
+	Output of my pass:
+	------------------
+	func @interchange_for_outer_parallelism(%arg0: memref<2048x2048x2048xf64>) {
+	    affine.for %arg1 = 0 to 2048 {
+	      affine.for %arg2 = 1 to 2048 {
+		affine.for %arg3 = 0 to 2048 {
+		  %0 = affine.load %arg0[%arg2, %arg1, %arg3] : memref<2048x2048x2048xf64>
+		  %1 = mulf %0, %0 : f64
+		  affine.store %1, %arg0[%arg2 - 1, %arg1, %arg3] : memref<2048x2048x2048xf64>
+		}
+	      }
+	    }
+	    return
+	  }
+
+	It is passed when first check statement is changed from "// CHECK-NEXT: affine.load %arg0[%arg2, %arg1, %arg3]"
+	to "// CHECK: affine.load %arg0[%arg2, %arg1, %arg3]". Instead of using CHECK_NEXT, CHECK is used.
+
+Test case 7: interchange_for_spatial_locality_mod
+-------------------------------------------------
+	Output of my pass:
+	-----------------
+	func @interchange_for_spatial_locality_mod(%arg0: memref<2048x2048x2048xf64>) {
+	    affine.for %arg1 = 0 to 2048 {
+	      affine.for %arg2 = 0 to 2048 {
+		affine.for %arg3 = 0 to 2048 {
+		  %0 = affine.load %arg0[%arg2 mod 2, %arg1, %arg3] : memref<2048x2048x2048xf64>
+		  affine.store %0, %arg0[%arg2 mod 2, %arg1, %arg3] : memref<2048x2048x2048xf64>
+		}
+	      }
+	    }
+	    return
+	  }
+
+	Check statements:
+	----------------
+	// CHECK:       affine.load %arg0[%{{.*}} mod 2, %arg1, %arg3] : memref<2048x2048x2048xf64>
+        // CHECK-NEXT:  affine.store %0, %arg0[%{{.*}} mod 2, %arg1, %arg3] : memref<2048x2048x2048xf64>
+
+	My output seems correct, but is not matched by the check statements.
diff --git a/mlir/lib/Transforms/Utils/LoopUtils.cpp b/mlir/lib/Transforms/Utils/LoopUtils.cpp
index dec0c4f7c4e..1da8fb009c4 100644
--- a/mlir/lib/Transforms/Utils/LoopUtils.cpp
+++ b/mlir/lib/Transforms/Utils/LoopUtils.cpp
@@ -32,6 +32,7 @@
 #include "llvm/ADT/SmallPtrSet.h"
 #include "llvm/Support/Debug.h"
 #include "llvm/Support/raw_ostream.h"
+#include <bits/stdc++.h>
 
 #define DEBUG_TYPE "LoopUtils"
 
@@ -702,6 +703,7 @@ bool mlir::isValidLoopInterchangePermutation(ArrayRef<AffineForOp> loops,
   unsigned maxLoopDepth = loops.size();
   std::vector<SmallVector<DependenceComponent, 2>> depCompsVec;
   getDependenceComponents(loops[0], maxLoopDepth, &depCompsVec);
+
   return checkLoopInterchangeDependences(depCompsVec, loops, loopPermMap);
 }
 
-- 
2.17.1

